
#!/usr/bin/env python3
"""
QSE Core Metrics Runner - Deep Autopoiesis Analysis (ENHANCED)
==============================================================

Generates comprehensive metrics for computational autopoiesis research with
improved robustness, reproducibility, and performance:

- Emergent time (Ï„) dynamics and self-organization
- Surplus field (S) statistics and viability maintenance
- Symbolic curvature (Ïƒ) evolution and meaning-making
- Rupture events and productive instability markers
- Quantum probability entropy and coherence measures
- Boundary dynamics and violation tracking
- Run-length analysis of stability/rupture cycles
- Autopoiesis gates for regression testing

ENHANCEMENTS:
âœ… Reproducible with seed control
âœ… Robust Ï„ saturation detection (no float jitter)
âœ… Fixed episode length counting
âœ… Improved zero-crossing detection
âœ… Buffered I/O for performance
âœ… Autopoiesis validation gates
âœ… Psi normalization error tracking

Usage:
    python qse_core_metrics.py --steps 1000 --out qse_metrics.jsonl --seed 123
    python qse_core_metrics.py --steps 5000 --out runs/detailed_qse.jsonl --verbose
"""

import json
import time
import argparse
import numpy as np
import random
from pathlib import Path
from collections import deque
from typing import Dict, List, Any

# Import QSE components
import sys
sys.path.append('.')
from emile_mini.config import QSEConfig
from emile_mini.qse_core import QSEEngine, calculate_symbolic_fields, calculate_emergent_time


class QSEMetricsCollector:
    """Comprehensive QSE metrics collection and analysis with enhanced robustness"""

    def __init__(self, config: QSEConfig = None):
        self.cfg = config or QSEConfig()
        self.engine = QSEEngine(self.cfg)

        # Tracking containers
        self.metrics_history = []
        self.rupture_episodes = []
        self.stability_episodes = []
        self.boundary_violations = []

        # Rolling windows for analysis
        self.tau_window = deque(maxlen=50)
        self.sigma_window = deque(maxlen=100)
        self.rupture_window = deque(maxlen=200)

        # State tracking
        self.current_episode = None
        self.episode_start = 0
        self.step_count = 0

    def collect_step_metrics(self, sigma: np.ndarray, dt: float) -> Dict[str, Any]:
        """Collect comprehensive metrics for a single QSE step with enhanced robustness"""

        # Execute QSE step
        qse_metrics = self.engine.step(sigma, dt)

        # Calculate additional derived metrics
        S = self.engine.S
        psi = self.engine.psi
        prob = np.abs(psi)**2

        # === EMERGENT TIME DYNAMICS (ENHANCED) ===
        eps = 1e-9  # Tolerance for float comparison
        tau_stats = {
            'tau_current': dt,
            'tau_min_bound': self.cfg.TAU_MIN,
            'tau_max_bound': self.cfg.TAU_MAX,
            'tau_at_min': dt <= self.cfg.TAU_MIN + eps,  # FIXED: Near-bound detection
            'tau_at_max': dt >= self.cfg.TAU_MAX - eps,  # FIXED: Near-bound detection
            'tau_utilization': (dt - self.cfg.TAU_MIN) / (self.cfg.TAU_MAX - self.cfg.TAU_MIN)
        }

        # === SURPLUS FIELD STATISTICS ===
        S_stats = {
            'S_mean': float(np.mean(S)),
            'S_std': float(np.std(S)),
            'S_min': float(np.min(S)),
            'S_max': float(np.max(S)),
            'S_range': float(np.max(S) - np.min(S)),
            'S_skewness': float(self._calculate_skewness(S)),
            'S_kurtosis': float(self._calculate_kurtosis(S)),
            'S_energy': float(np.sum(S**2)),
            'S_gradient_norm': float(np.mean(np.abs(np.gradient(S)))),
        }

        # === BOUNDARY DYNAMICS (with soft barrier option) ===
        eps_boundary = 1e-12
        S_violations = ((S < -eps_boundary) | (S > 1.0 + eps_boundary)).sum()
        S_clamps = (np.isclose(S, 0.0, atol=eps_boundary) | np.isclose(S, 1.0, atol=eps_boundary)).sum()

        # SUGGESTION: Add soft barriers in qse_core.py update_surplus():
        # S_new = np.clip(S_raw, 0.0, 1.0)  # Hard clipping (most robust)
        # OR: S_new = 0.5 * (1 + np.tanh(2*(S_raw - 0.5)))  # Soft sigmoid boundaries

        boundary_stats = {
            'S_violation_count': int(S_violations),
            'S_violation_rate': float(S_violations / S.size),
            'S_clamp_count': int(S_clamps),
            'S_clamp_rate': float(S_clamps / S.size),
            'S_boundary_pressure': float(np.sum(np.maximum(0, S - 1.0) + np.maximum(0, -S)))
        }

        # === SYMBOLIC CURVATURE ANALYSIS (ENHANCED) ===
        sigma_stats = {
            'sigma_mean': float(np.mean(sigma)),
            'sigma_std': float(np.std(sigma)),
            'sigma_min': float(np.min(sigma)),
            'sigma_max': float(np.max(sigma)),
            'sigma_abs_mean': float(np.mean(np.abs(sigma))),
            'sigma_positive_fraction': float(np.mean(sigma > 0)),
            'sigma_negative_fraction': float(np.mean(sigma < 0)),
            'sigma_zero_crossings': int(self._count_zero_crossings(sigma)),  # ENHANCED
            'sigma_local_maxima': int(self._count_local_extrema(sigma, 'max')),
            'sigma_local_minima': int(self._count_local_extrema(sigma, 'min'))
        }

        # === RUPTURE ANALYSIS (TONIC vs PHASIC) ===
        rupture_threshold = self.cfg.S_THETA_RUPTURE
        rupture_mask = np.abs(sigma) > rupture_threshold
        rupture_fraction = float(np.mean(rupture_mask))

        # TONIC RUPTURE: Any cell above threshold (healthy "background crackle")
        tonic_rupture_active = rupture_fraction > 0

        # PHASIC RUPTURE: Coherent spatial patterns + temporal persistence
        MIN_FRAC_STRONG = 0.12  # Lowered from 0.15 â†’ 0.12 (12% of cells)
        DWELL_STRONG = 2        # Lowered from 3 â†’ 2 steps for more sensitivity

        # Strong rupture detection
        strong_rupture_now = rupture_fraction >= MIN_FRAC_STRONG

        # Apply temporal persistence filter
        if not hasattr(self, '_strong_rupture_dwell_count'):
            self._strong_rupture_dwell_count = 0
            self._strong_rupture_state = False

        if strong_rupture_now:
            self._strong_rupture_dwell_count += 1
            if self._strong_rupture_dwell_count >= DWELL_STRONG:
                self._strong_rupture_state = True
        else:
            self._strong_rupture_dwell_count = max(0, self._strong_rupture_dwell_count - 1)
            if self._strong_rupture_dwell_count == 0:
                self._strong_rupture_state = False

        # Largest coherent cluster analysis
        largest_cluster_size = self._find_largest_cluster(rupture_mask)

        rupture_stats = {
            # TONIC RUPTURE (background aliveness)
            'tonic_rupture_active': tonic_rupture_active,
            'tonic_rupture_fraction': rupture_fraction,
            'tonic_rupture_intensity': float(np.mean(np.abs(sigma[rupture_mask]))) if rupture_fraction > 0 else 0.0,

            # PHASIC RUPTURE (coherent regime shifts)
            'phasic_rupture_active': self._strong_rupture_state,
            'phasic_rupture_fraction': rupture_fraction if strong_rupture_now else 0.0,
            'phasic_rupture_spatial_coherence': float(largest_cluster_size / sigma.size),
            'phasic_rupture_dwell_count': self._strong_rupture_dwell_count,

            # SHARED METRICS
            'rupture_threshold': rupture_threshold,
            'largest_cluster_size': int(largest_cluster_size),
            'total_ruptured_cells': int(np.sum(rupture_mask))
        }

        # === QUANTUM COHERENCE (ENHANCED) ===
        # prob already calculated above - no need to recompute

        # Convert density -> probability mass using grid spacing
        if hasattr(self.engine, "x") and len(self.engine.x) >= 2:
            dx = float(self.engine.x[1] - self.engine.x[0])
        else:
            dx = 1.0 / len(prob)  # fallback if x not exposed

        p = prob * dx
        p_sum = p.sum() + 1e-16
        p = p / p_sum  # now a proper probability mass function

        prob_entropy = -np.sum(p * np.log(p + 1e-16))
        max_entropy = np.log(len(p))

        # Participation metrics:
        # - "prob_concentration": sum p^2 (high = concentrated)
        # - "participation_ratio": 1/sum p^2 (classic IPR)
        concentration = float(np.sum(p**2))
        participation_ratio = float(1.0 / (np.sum(p**2) + 1e-16))

        quantum_stats = {
            'prob_entropy': float(prob_entropy),
            'prob_entropy_normalized': float(prob_entropy / max_entropy),  # âˆˆ [0,1]
            'prob_max': float(np.max(p)),                                  # âˆˆ (0,1]
            'prob_concentration': concentration,                           # âˆˆ [1/N,1]
            'participation_ratio': participation_ratio,                    # âˆˆ [1, N]
            'wavefunction_energy': float(np.sum(prob) * dx),               # â‰ˆ 1
            'psi_norm_error': float(abs(np.sum(prob) * dx - 1.0)),
            'quantum_coupling': float(self.cfg.QUANTUM_COUPLING)
        }

        # === TEMPORAL PATTERNS ===
        self.tau_window.append(dt)
        self.sigma_window.append(np.mean(sigma))
        self.rupture_window.append(rupture_fraction)

        temporal_stats = {
            'tau_trend': self._calculate_trend(list(self.tau_window)),
            'sigma_trend': self._calculate_trend(list(self.sigma_window)),
            'rupture_trend': self._calculate_trend(list(self.rupture_window)),
            'tau_autocorr': self._calculate_autocorr(list(self.tau_window)),
            'sigma_autocorr': self._calculate_autocorr(list(self.sigma_window))
        }

        # === EPISODE TRACKING (FIXED - Using PHASIC rupture) ===
        episode_stats = self._update_episode_tracking(rupture_stats['phasic_rupture_active'])

        # Combine all metrics
        step_metrics = {
            'step': self.step_count,
            'timestamp': time.time(),
            'dt': dt,
            **tau_stats,
            **S_stats,
            **boundary_stats,
            **sigma_stats,
            **rupture_stats,
            **quantum_stats,
            **temporal_stats,
            **episode_stats
        }

        self.metrics_history.append(step_metrics)
        self.step_count += 1

        return step_metrics

    def _calculate_skewness(self, data: np.ndarray) -> float:
        """Calculate skewness (third moment)"""
        mean = np.mean(data)
        std = np.std(data)
        if std == 0:
            return 0.0
        return float(np.mean(((data - mean) / std) ** 3))

    def _calculate_kurtosis(self, data: np.ndarray) -> float:
        """Calculate kurtosis (fourth moment)"""
        mean = np.mean(data)
        std = np.std(data)
        if std == 0:
            return 0.0
        return float(np.mean(((data - mean) / std) ** 4)) - 3.0

    def _find_largest_cluster(self, mask: np.ndarray) -> int:
        """Find the size of the largest connected cluster in a 1D mask"""
        if not np.any(mask):
            return 0

        # Simple 1D connected components
        clusters = []
        current_cluster_size = 0

        for cell in mask:
            if cell:
                current_cluster_size += 1
            else:
                if current_cluster_size > 0:
                    clusters.append(current_cluster_size)
                    current_cluster_size = 0

        # Don't forget the last cluster if it extends to the end
        if current_cluster_size > 0:
            clusters.append(current_cluster_size)

        return max(clusters) if clusters else 0

    def _count_zero_crossings(self, data: np.ndarray) -> int:
        """ENHANCED: Count zero crossings with better handling of exact zeros"""
        if len(data) < 2:
            return 0
        # Use signbit for more robust detection
        s = np.signbit(data)
        return int(np.sum(s[1:] != s[:-1]))
        """ENHANCED: Count zero crossings with better handling of exact zeros"""
        if len(data) < 2:
            return 0
        # Use signbit for more robust detection
        s = np.signbit(data)
        return int(np.sum(s[1:] != s[:-1]))

    def _count_local_extrema(self, data: np.ndarray, extrema_type: str) -> int:
        """Count local maxima or minima"""
        if len(data) < 3:
            return 0

        if extrema_type == 'max':
            extrema = ((data[1:-1] > data[:-2]) & (data[1:-1] > data[2:]))
        else:  # min
            extrema = ((data[1:-1] < data[:-2]) & (data[1:-1] < data[2:]))

        return int(np.sum(extrema))

    def _find_largest_cluster(self, mask: np.ndarray) -> int:
        """Find the size of the largest connected cluster in a 1D mask"""
        if not np.any(mask):
            return 0

        # Simple 1D connected components
        clusters = []
        current_cluster_size = 0

        for cell in mask:
            if cell:
                current_cluster_size += 1
            else:
                if current_cluster_size > 0:
                    clusters.append(current_cluster_size)
                    current_cluster_size = 0

        # Don't forget the last cluster if it extends to the end
        if current_cluster_size > 0:
            clusters.append(current_cluster_size)

        return max(clusters) if clusters else 0

    def _calculate_trend(self, data: List[float]) -> float:
        """Calculate linear trend slope"""
        if len(data) < 2:
            return 0.0

        x = np.arange(len(data))
        y = np.array(data)

        # Check for sufficient variance
        if np.std(x) < 1e-12 or np.std(y) < 1e-12:
            return 0.0

        # Linear regression slope with warning suppression
        with np.errstate(divide='ignore', invalid='ignore'):
            corr_matrix = np.corrcoef(x, y)
            if corr_matrix.shape == (2, 2) and not np.isnan(corr_matrix[0, 1]):
                slope = corr_matrix[0, 1] * (np.std(y) / np.std(x))
            else:
                slope = 0.0
        return float(slope) if not np.isnan(slope) else 0.0

    def _calculate_autocorr(self, data: List[float], lag: int = 1) -> float:
        """Calculate autocorrelation at given lag"""
        if len(data) <= lag:
            return 0.0

        y = np.array(data)
        y = y - np.mean(y)

        # Check for sufficient variance
        if np.std(y) < 1e-12:
            return 0.0

        # Autocorrelation with warning suppression
        with np.errstate(divide='ignore', invalid='ignore'):
            corr_matrix = np.corrcoef(y[:-lag], y[lag:])
            if corr_matrix.shape == (2, 2) and not np.isnan(corr_matrix[0, 1]):
                autocorr = corr_matrix[0, 1]
            else:
                autocorr = 0.0
        return float(autocorr) if not np.isnan(autocorr) else 0.0

    def _update_episode_tracking(self, is_rupture: bool) -> Dict[str, Any]:
        """Track stability/rupture episodes with FIXED length calculation"""

        if self.current_episode is None:
            # Start first episode
            self.current_episode = 'rupture' if is_rupture else 'stability'
            self.episode_start = self.step_count

        elif (self.current_episode == 'rupture') != is_rupture:
            # Episode transition
            episode_length = self.step_count - self.episode_start

            episode_record = {
                'type': self.current_episode,
                'start_step': self.episode_start,
                'end_step': self.step_count - 1,
                'length': episode_length
            }

            if self.current_episode == 'rupture':
                self.rupture_episodes.append(episode_record)
            else:
                self.stability_episodes.append(episode_record)

            # Start new episode
            self.current_episode = 'rupture' if is_rupture else 'stability'
            self.episode_start = self.step_count

        return {
            'current_episode_type': self.current_episode,
            'current_episode_length': self.step_count - self.episode_start + 1,  # FIXED: Inclusive length
            'total_rupture_episodes': len(self.rupture_episodes),
            'total_stability_episodes': len(self.stability_episodes),
            'avg_rupture_length': np.mean([ep['length'] for ep in self.rupture_episodes]) if self.rupture_episodes else 0.0,
            'avg_stability_length': np.mean([ep['length'] for ep in self.stability_episodes]) if self.stability_episodes else 0.0
        }

    def generate_summary_report(self) -> Dict[str, Any]:
        """Generate comprehensive summary with autopoiesis gates"""

        if not self.metrics_history:
            return {}

        # Extract time series
        tau_series = [m['tau_current'] for m in self.metrics_history]
        S_means = [m['S_mean'] for m in self.metrics_history]
        sigma_means = [m['sigma_mean'] for m in self.metrics_history]

        # TONIC vs PHASIC rupture separation
        tonic_rupture_fractions = [m['tonic_rupture_fraction'] for m in self.metrics_history]
        phasic_rupture_events = [m['phasic_rupture_active'] for m in self.metrics_history]

        violation_rates = [m['S_violation_rate'] for m in self.metrics_history]
        psi_norm_errors = [m['psi_norm_error'] for m in self.metrics_history]

        # Summary statistics
        summary = {
            'experiment_metadata': {
                'total_steps': len(self.metrics_history),
                'config_summary': {
                    'TAU_MIN': self.cfg.TAU_MIN,
                    'TAU_MAX': self.cfg.TAU_MAX,
                    'S_THETA_RUPTURE': self.cfg.S_THETA_RUPTURE,
                    'QUANTUM_COUPLING': self.cfg.QUANTUM_COUPLING,
                    'GRID_SIZE': self.cfg.GRID_SIZE
                }
            },

            'emergent_time_dynamics': {
                'tau_mean': float(np.mean(tau_series)),
                'tau_std': float(np.std(tau_series)),
                'tau_min': float(np.min(tau_series)),
                'tau_max': float(np.max(tau_series)),
                'tau_at_min_fraction': float(np.mean([m['tau_at_min'] for m in self.metrics_history])),
                'tau_at_max_fraction': float(np.mean([m['tau_at_max'] for m in self.metrics_history])),
                'tau_utilization_mean': float(np.mean([m['tau_utilization'] for m in self.metrics_history])),
                'tau_trend_slope': self._calculate_trend(tau_series)
            },

            'surplus_field_statistics': {
                'S_mean_overall': float(np.mean(S_means)),
                'S_std_overall': float(np.std(S_means)),
                'S_range_overall': float(np.max(S_means) - np.min(S_means)),
                'S_energy_mean': float(np.mean([m['S_energy'] for m in self.metrics_history])),
                'S_gradient_norm_mean': float(np.mean([m['S_gradient_norm'] for m in self.metrics_history]))
            },

            'boundary_violation_analysis': {
                'violation_rate_mean': float(np.mean(violation_rates)),
                'violation_rate_max': float(np.max(violation_rates)),
                'steps_with_violations': int(np.sum([r > 0 for r in violation_rates])),
                'violation_fraction': float(np.mean([r > 0 for r in violation_rates])),
                'clamp_rate_mean': float(np.mean([m['S_clamp_rate'] for m in self.metrics_history]))
            },

            'symbolic_curvature_dynamics': {
                'sigma_mean_overall': float(np.mean(sigma_means)),
                'sigma_std_overall': float(np.std(sigma_means)),
                'sigma_abs_mean_overall': float(np.mean([m['sigma_abs_mean'] for m in self.metrics_history])),
                'sigma_positive_fraction_mean': float(np.mean([m['sigma_positive_fraction'] for m in self.metrics_history])),
                'zero_crossings_total': int(np.sum([m['sigma_zero_crossings'] for m in self.metrics_history]))
            },

            'rupture_episode_analysis': {
                # TONIC RUPTURE (background aliveness)
                'tonic_rupture_fraction_mean': float(np.mean(tonic_rupture_fractions)),
                'tonic_rupture_fraction_std': float(np.std(tonic_rupture_fractions)),
                'tonic_rupture_time_fraction': float(np.mean([m['tonic_rupture_active'] for m in self.metrics_history])),
                'tonic_rupture_intensity_mean': float(np.mean([m['tonic_rupture_intensity'] for m in self.metrics_history])),

                # PHASIC RUPTURE (coherent regime shifts)
                'phasic_rupture_events': int(np.sum(phasic_rupture_events)),
                'phasic_rupture_time_fraction': float(np.mean(phasic_rupture_events)),
                'phasic_rupture_episode_count': len(self.rupture_episodes),
                'avg_phasic_episode_length': float(np.mean([ep['length'] for ep in self.rupture_episodes])) if self.rupture_episodes else 0.0,

                # EPISODE STRUCTURE
                'total_stability_episodes': len(self.stability_episodes),
                'avg_stability_episode_length': float(np.mean([ep['length'] for ep in self.stability_episodes])) if self.stability_episodes else 0.0,
                'max_coherent_cluster_fraction': float(np.max([m['phasic_rupture_spatial_coherence'] for m in self.metrics_history]))
            },

            'quantum_coherence_metrics': {
                'entropy_mean': float(np.mean([m['prob_entropy_normalized'] for m in self.metrics_history])),
                'entropy_std': float(np.std([m['prob_entropy_normalized'] for m in self.metrics_history])),
                'concentration_mean': float(np.mean([m['prob_concentration'] for m in self.metrics_history])),
                'participation_ratio_mean': float(np.mean([m['participation_ratio'] for m in self.metrics_history])),
                'max_prob_mean': float(np.mean([m['prob_max'] for m in self.metrics_history])),
                'psi_norm_error_mean': float(np.mean(psi_norm_errors))  # NEW: Normalization tracking
            },

            'autopoiesis_markers': {
                'self_organization_index': float(np.std(tau_series) / np.mean(tau_series)) if np.mean(tau_series) > 0 else 0.0,
                'viability_maintenance_score': 1.0 - float(np.mean(violation_rates)),
                'productive_instability_balance': float(np.mean(phasic_rupture_events)),  # Use phasic, not tonic
                'tonic_aliveness_score': float(np.mean([m['tonic_rupture_active'] for m in self.metrics_history])),  # NEW: Health metric
                'boundary_integrity_score': 1.0 - float(np.mean([m['S_clamp_rate'] for m in self.metrics_history])),
                'temporal_autonomy_index': float(1.0 - np.mean([m['tau_at_max'] for m in self.metrics_history]))
            }
        }

        # NEW: Autopoiesis gates for CI/regression testing (TONIC vs PHASIC)
        gates = {
            'tau_bounds': (summary['emergent_time_dynamics']['tau_min'] >= self.cfg.TAU_MIN - 1e-9 and
                          summary['emergent_time_dynamics']['tau_max'] <= self.cfg.TAU_MAX + 1e-9),
            'S_violations_low': summary['boundary_violation_analysis']['violation_rate_max'] <= 5e-2,  # Relaxed for numerical overshoots
            'tau_saturation': summary['emergent_time_dynamics']['tau_at_max_fraction'] <= 0.30,
            'tonic_aliveness': summary['rupture_episode_analysis']['tonic_rupture_time_fraction'] >= 0.80,  # NEW: Background crackle
            'phasic_rupture_range': 0.002 <= summary['rupture_episode_analysis']['phasic_rupture_time_fraction'] <= 0.30,  # Lowered min from 0.05 â†’ 0.002
            'psi_normalized': summary['quantum_coherence_metrics']['psi_norm_error_mean'] <= 1e-10,
            'entropy_bounded': 0.75 <= summary['quantum_coherence_metrics']['entropy_mean'] <= 0.98  # Healthy high entropy
        }

        summary['autopoiesis_gates'] = gates

        return summary


def run_qse_metrics_collection(steps: int = 1000, output_file: str = None, verbose: bool = False, seed: int = 0) -> str:
    """Run comprehensive QSE metrics collection with enhanced features"""

    # NEW: Reproducibility - seed all random generators
    random.seed(seed)
    np.random.seed(seed)

    print(f"ðŸ”¬ Starting QSE Core Metrics Collection (ENHANCED)")
    print(f"ðŸ“Š Steps: {steps}, Seed: {seed}")

    # Initialize collector
    collector = QSEMetricsCollector()

    # Storage for JSONL output
    if output_file:
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

    # Run collection with BUFFERED I/O
    sigma_prev = None
    start_time = time.time()

    # ENHANCED: Buffered file writing for performance
    file_handle = None
    if output_file:
        file_handle = open(output_file, 'w')

    try:
        for step in range(steps):
            # Generate sigma from current surplus
            psi, phi, sigma = calculate_symbolic_fields(collector.engine.S, collector.cfg)

            # Calculate emergent time
            if sigma_prev is not None:
                tau = calculate_emergent_time(sigma, sigma_prev, collector.cfg)
            else:
                tau = collector.cfg.TAU_MAX

            # Collect metrics
            step_metrics = collector.collect_step_metrics(sigma, tau)

            # Write to JSONL with buffering
            if file_handle:
                file_handle.write(json.dumps(step_metrics) + '\n')
                if step % 200 == 0:  # Flush periodically
                    file_handle.flush()

            # Progress reporting
            if verbose and (step % 100 == 0 or step < 10):
                print(f"  Step {step:5d}: Ï„={tau:.3f}, rupture={step_metrics['tonic_rupture_fraction']:.2f}, S_mean={step_metrics['S_mean']:.3f}")

            sigma_prev = sigma.copy()

    finally:
        if file_handle:
            file_handle.close()

    # Generate summary
    summary = collector.generate_summary_report()

    elapsed = time.time() - start_time
    print(f"\nâœ… Collection complete! {steps} steps in {elapsed:.2f}s (seed: {seed})")

    # NEW: Print autopoiesis gates
    if summary and 'autopoiesis_gates' in summary:
        print(f"ðŸ§ª Autopoiesis Gates:")
        gates = summary['autopoiesis_gates']
        for gate_name, passed in gates.items():
            status = "âœ… PASS" if passed else "âŒ FAIL"
            print(f"  {gate_name}: {status}")

    print(f"ðŸ“ˆ Autopoiesis markers:")
    if summary:
        markers = summary['autopoiesis_markers']
        print(f"  Self-organization: {markers['self_organization_index']:.3f}")
        print(f"  Viability maintenance: {markers['viability_maintenance_score']:.3f}")
        print(f"  Productive instability: {markers['productive_instability_balance']:.3f}")
        print(f"  Boundary integrity: {markers['boundary_integrity_score']:.3f}")
        print(f"  Temporal autonomy: {markers['temporal_autonomy_index']:.3f}")

    # Save summary with seed info
    if output_file:
        summary_file = output_file.replace('.jsonl', '_summary.json')
        if summary:
            summary.setdefault('experiment_metadata', {}).update({'seed': seed})
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        print(f"ðŸ“‹ Summary saved: {summary_file}")

        return summary_file

    return json.dumps(summary, indent=2)


def main():
    parser = argparse.ArgumentParser(description='QSE Core Metrics Collection (Enhanced)')
    parser.add_argument('--steps', type=int, default=1000, help='Number of QSE steps to run')
    parser.add_argument('--out', type=str, default='qse_core_metrics.jsonl', help='Output JSONL file')
    parser.add_argument('--seed', type=int, default=0, help='RNG seed for reproducibility')  # NEW
    parser.add_argument('--verbose', action='store_true', help='Verbose progress reporting')

    args = parser.parse_args()

    summary_file = run_qse_metrics_collection(
        steps=args.steps,
        output_file=args.out,
        verbose=args.verbose,
        seed=args.seed  # NEW
    )

    print(f"\nðŸŽ¯ Enhanced metrics collection complete!")
    print(f"ðŸ“„ JSONL data: {args.out}")
    print(f"ðŸ“‹ Summary: {summary_file}")
    print(f"ðŸ”„ Reproducible with: --seed {args.seed}")


if __name__ == "__main__":
    main()
